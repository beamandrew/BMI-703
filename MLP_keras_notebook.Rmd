---
title: "MLP + Keras Tutorial"
output: html_notebook
---

In this notebook we're going to cover the basics of implementing a neural network using modern tools and techniques. We're going to use the R [wrapper](https://keras.rstudio.com) of [keras](https://keras.rstudio.com). Keras is itself a high-level python wrapper for several backend libraries that allow for construction of computational graphs, but the one we will be using is [tensorflow](https://www.tensorflow.org) which is developed and maintained by Google.

Using a library like tensorflow comes with several advantages, including:

- Automatic differentiation
- Numerically stable versions of functions like `exp`, `sigmoid`, and `softmax`
- Ability to run on a variety of platforms (Linux, OS X, Windows, Android, iOS, etc)
- Transparent use of additional compute resources such as a GPU

First, we're going to load the keras package and the exact same data as before:
```{r, eval=FALSE}
library(keras)
load(url("https://www.dropbox.com/s/unmfitketiba9i7/cat_dog_data_greyscale.RData?dl=1"))
## Flatten all of our images into row-vectors ##
x_train <- matrix(x_train,nrow=nrow(x_train))
x_test <- matrix(x_test,nrow=nrow(x_test))
num_features <- ncol(x_train)
```

Now let's reimplement the perceptron using keras to get a feel for the syntax. The first thing we need to do is to write some boilerplate code to initialize the model:
```{r, eval=FALSE}
model <- keras_model_sequential() 
```
We are going to use _pipes_ which were first made popular by the [magrittr](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html) package. If you haven't seen this before it can bee a little confusing, but the syntax looks like this: `left_arg %>% right_function` where `left_arg` is some kind of data (such as a data frame that could have itself been generated by a function) that will be passed as the _first_ argument to `right_function`. The equivalent functional version would be `right_function(left_arg)`. Piping makes it easier to chain to together a long series of transformations, especially when you have a lot of operations, because the corresponding functional version would have a lot of nested calls that would be difficult to read. 

The primary layer we will be using for now is the [`layer_dense`](https://keras.rstudio.com/reference/layer_dense.html). The main parameter for `layer_dense` is the number of hidden units (`units`) it should have. Each unit will perform a weighted combination of all the inputs from the previous layer followed by the application of an activation function (`activation`). In the case of our perceptron, this means we just want one hidden unit followed by a sigmoid activation. It looks something like this:
```{r, eval=FALSE}
model %>%
  layer_dense(units = 1, input_shape = c(num_features), activation = 'sigmoid')
```

Since this is the first (and only) layer in the network, we needed to tell it how large the input vector will be. Notice that we don't have to tell it how many samples to expect (this will be automatically inferred), but only how many features we have. The next step is to _compile_ the model. This is where the computational graph specified by our model will be compiled along with a loss function so that gradients can be computed. We also must specify how we would like to perform the optimization. For now we will use vanilla stochastic gradient descent, but check the [reference](https://keras.rstudio.com/reference/index.html) for other options. 
```{r, eval=FALSE}
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_sgd(lr = 0.01),
  metrics = c('accuracy')
)
```

Now we are finally ready fit the model: 
```{r}
model %>% fit(
  x_train, y_train, verbose=1,
  validation_data = list(x_test,y_test),
  epochs = 100, batch_size = 128
)
```
Note that `model` is modified _in place_ so we do not need to assign the result to a variable.

## In Class Assignment 
Now it's time for some *grad student descent*. Your goal is to get the best value for the loss function on the *validation* data. Keep track of your best validation loss and validation accuracy (along with your settings) and we'll see who was able to find the best model.  Here are some suggestions to get you started:

- Add one or more hidden layers.
- Change the parameters for the hidden layers. Here are some ones to start with:
  - Try different activation functions. Some suggestions include 'relu' and 'elu' but there are many others.
  - Add regularization (such as _L1_ or _L2_) to the `kernel_regularizer`.
  - Check out [`layer_dense`](https://keras.rstudio.com/reference/layer_dense.html) for more options.
- Add [_dropout_](https://keras.rstudio.com/reference/layer_dropout.html) in between your dense layers.
- Use a different optimizer and/or a different learning rate

Fill in the template below to improve the model:
```{r, eval=FALSE}
model %>%
  ## Insert your improvments here
  
  layer_dense(units = 1, activation = 'sigmoid')
```

